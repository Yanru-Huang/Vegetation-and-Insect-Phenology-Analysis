{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d81d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Environmental_factors_impact_analysis###\n",
    "\n",
    "#This code is designed to analyse the sensitivity and contribution of environmental factors to vegetation phenology (IOD) and insect phenology (LUD) using partial correlation, ridge regression, and random forest methods.\n",
    "#Environmental factors include: total_evaporation_sum(evapotranspiration), hu(average relative humidity), rr(precipitation), qq(radiation), tg(average temperature), fg(average wind speed), volumetric_soil_water_layer_(soil moisture), soil_temperature_level_1(soil temperature).\n",
    "\n",
    "#Step:\n",
    "#(1) Data Matching: Obtain environmental factors for the 0 to 6 months preceding the IOD and LUD records, and calculate the mean values of these environmental factors for the periods 0 to 1, ..., 6 months.\n",
    "#(2) Geographic Zoning: Considering the geographical variations in environmental factors' impacts, we categorise records into 5째x5째 zones, moving 1째 each time in latitude and longitude, to perform statistical analyses (only retaining results where a single grid contains data for more than 10 species with over 200 records).\n",
    "#(3) Optimal Pre-Season Selection: For each phenological pattern, after removing inter-annual trends using linear regression, we conduct correlation analyses between LUD/IOD and environmental factors to identify the period with the highest absolute correlation as the optimal pre-season.\n",
    "#(4) Partial Correlation/Ridge Regression/Random Forest Analysis: Using the optimal pre-season values of environmental variables, after removing yearly trends and standardizing, we execute the respective analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd289f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load package\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import pingouin as pg\n",
    "import os,shutil\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c3d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data match\n",
    "# Batch obtain data from .npy environmental data files in a folder\n",
    "def list_npy_files(root_dir, npy_list):\n",
    "    # Use glob to find all .npy files in the directory and its subdirectories\n",
    "    npy_files = glob.glob(os.path.join(root_dir, '**/*.npy'), recursive=True)\n",
    "    npy_list.extend(npy_files)\n",
    "\n",
    "# Initialise an empty list\n",
    "list_npy = []\n",
    "\n",
    "# Store result in list_csv\n",
    "#list_csv_files(r\"...:\\Environmental_factors_file_path\", list_csv) #Modified to the path where environmental factors are stored \n",
    "list_npy_files(r\"...\\lud_sample\", list_npy) #Modified to the path where environmental factors are stored \n",
    "\n",
    "# Read the phenology records\n",
    "df = pd.read_hdf('.../pts.h5', key='data')\n",
    "df = df.reset_index()\n",
    "\n",
    "# Match phenological data and environmental factor data\n",
    "for file in list_npy:\n",
    "    fpath,fname=os.path.split(file)\n",
    "    fname = fname.split('.')[0]\n",
    "    env = np.load(file)\n",
    "    # Creat DataFrame\n",
    "    env=pd.DataFrame(env)\n",
    "    # Calculate the average environmental factors between 0 and 6 months\n",
    "    env2 = np.cumsum(env, axis=1) / (np.arange(7)+1)\n",
    "    # Add the specified column name to the column\n",
    "    columns = [fname+'_{}'.format(i) for i in range(0, 7)]\n",
    "    env2.columns = columns\n",
    "    df = df.join(env2)\n",
    "\n",
    "# Adding the column order_cluster facilitates subsequent analysis\n",
    "df = df.assign(species_cluster=df['species'].astype(str) + '_' + df['Cluster'].astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa179e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using linear regression to remove annual trends\n",
    "def remove_annual_trends(data, column):\n",
    "    X = data['year']\n",
    "    y = data[column]\n",
    "    model = OLS(y, sm.add_constant(X))\n",
    "    results = model.fit()\n",
    "    y_residual = results.resid\n",
    "    return y_residual\n",
    "\n",
    "\n",
    "# Optimal Pre-Season Selection\n",
    "# Using linear regression to remove annual trends\n",
    "def calculate_most_influential_column(factor, filtered_df1):\n",
    "    # Create a dictionary to store the partial correlation coefficients for each factor column\n",
    "    partial_corr = {}\n",
    "    # Calculate the partial correlation coefficient for each factor column\n",
    "    for i in range(7):\n",
    "        column_name = factor + str(i)\n",
    "        filtered_df1 =  filtered_df1.dropna(subset=[column_name])\n",
    "        # Remove annual trends\n",
    "        y_residual = remove_annual_trends(filtered_df1,column_name)\n",
    "        # Calculate the partial correlation coefficient between the residuals of the factor column and the LUD/IOD column\n",
    "        partial_corr[column_name] = pearsonr(y_residual, filtered_df1[\"LUD\"])[0] #LUD/IOD\n",
    "\n",
    "    # Convert the partial correlation coefficients to a Series and sort it (based on absolute values)\n",
    "    partial_corr_series = pd.Series(partial_corr)\n",
    "    # Preserve the non-absolute values\n",
    "    \n",
    "    partial_corr_series = partial_corr_series.abs().sort_values(ascending=False)\n",
    "    \n",
    "    # The most correlated (the column name with the highest absolute value of the partial correlation coefficient)\n",
    "    most_influential_column = partial_corr_series.idxmax()\n",
    "\n",
    "    return most_influential_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c87553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Correlation\n",
    "\n",
    "# Environmental factors list\n",
    "column_list =[\"fg_\",\"hu_\",\"rr_\",\"qq_\",\"tg_\",\"total_evaporation_sum_\",\"volumetric_soil_water_layer_1_\",'soil_temperature_level_1_']\n",
    "\n",
    "# Geographic Zoning\n",
    "for s in range(1,5): # Move 1째 each time \n",
    "    mean_r=[]  \n",
    "    mean_r2=[] \n",
    "    for lon in range(-15,35,5):\n",
    "        for lat in range(30,75,5):\n",
    "            # Select the phenology data of the corresponding area\n",
    "            filtered_df = df.query(f'{lon+s} <= Longitude <= {lon+5+s} and {lat} <= Latitude <= {lat+5}').reset_index() \n",
    "            #filtered_df = df.query(f'{lon} <= Longitude <= {lon+5} and {lat+s} <= Latitude <= {lat+5+s}').reset_index() #Replace 's' with 'lat' after iteration ends and run again\n",
    "            # Count the number of records (>200) of phenological patterns in the geographical grid\n",
    "            number_of_categories = (filtered_df.groupby('species_cluster')\n",
    "                                    .size()\n",
    "                                    .reset_index(name='count')\n",
    "                                    .query('count > 200')\n",
    "                                    .shape[0])\n",
    "            # If the required number of phenological patterns >= 10\n",
    "            if number_of_categories>=10:\n",
    "                #Iterate over each phenological pattern\n",
    "                all_list =[]\n",
    "                species_list = filtered_df['species_cluster'].unique()\n",
    "                scaler = StandardScaler()\n",
    "                for sp in species_list:\n",
    "                    MIC_list=[] \n",
    "                    filtered_df1=filtered_df[filtered_df[\"species_cluster\"]==sp].copy()\n",
    "                    if(filtered_df1['year'].nunique()>=10): #Filter phenological patterns with records >= 10 years\n",
    "                        for factor in column_list:  \n",
    "                            MIC = calculate_most_influential_column(factor, filtered_df1)\n",
    "                            MIC_list.append(MIC)#Record the optimal pre-season period for each environmental factor \n",
    "                        filtered_df1 =  filtered_df1.dropna(subset=MIC_list)\n",
    "                        for fc in MIC_list:\n",
    "                            # Remove annual trends\n",
    "                            y_residual = remove_annual_trends(filtered_df1,fc)\n",
    "                            # Standardise residuals\n",
    "                            filtered_df1[fc] = scaler.fit_transform(y_residual.values.reshape(-1, 1)) \n",
    "                        pc_list =[]\n",
    "                        # Calculate partial correlation\n",
    "                        for factor in MIC_list:\n",
    "                            covar_factors = MIC_list.copy()\n",
    "                            covar_factors.remove(factor)\n",
    "                            partial_corr = pg.partial_corr(data=filtered_df1, x='LUD', y=factor, covar=covar_factors) #LUD IOD\n",
    "                            # Record the results\n",
    "                            pc_list.append([partial_corr['r'][0],partial_corr['p-val'][0]])\n",
    "                        all_list.append([sp,filtered_df1[\"order\"].unique()[0],pc_list])\n",
    "                \n",
    "                # Organise data\n",
    "                df_all = pd.DataFrame(all_list, columns=['sp', 'order', 'params'])\n",
    "                for i, fa in enumerate(column_list):\n",
    "                    df_all[[fa + 'r', fa + 'p']] = pd.DataFrame(df_all['params'].tolist(), index=df_all.index)[i].tolist()\n",
    "                # Calculate the average partial correlation coefficients for a single geographical grid\n",
    "                columns_to_average = [f\"{fa}r\" for fa in column_list]\n",
    "                mean_r.append([lon + 2.5 + s, lat + 2.5, df_all[columns_to_average].mean().tolist()])\n",
    "                # Calculate the average partial correlation coefficients (significant P<0.05) for a single geographical grid\n",
    "                mean_r2.append([lon + 2.5 + s, lat + 2.5, [df_all[df_all[f\"{fa}p\"] < 0.05][f\"{fa}r\"].mean() for fa in column_list]])\n",
    "    \n",
    "    # Organise and output data\n",
    "    all_list_grid = [item[:2] + item[2] for item in mean_r]  \n",
    "    columns = [\"lon\",\"lat\",\"fg_r\",\"hu_r\",\"rr_r\",\"qq_r\",\"tg_r\",\"total_evaporation_sum_r\",\"volumetric_soil_water_layer_1_r\",'soil_temperature_level_1_r']\n",
    "    all_list_grid = pd.DataFrame(all_list_grid, columns=columns)\n",
    "    all_list_grid.to_csv(\"...\\\\LUD_PC_r\" + str(s) + \".csv\", index=False) \n",
    "    #all_list_grid.to_csv(\"...\\\\LUD_PC_n\" + str(s) + \".csv\", index=False) #When replaced with 'lat + s', output as 'n'\n",
    "    all_list_grid2 = [item[:2] + item[2] for item in mean_r2] \n",
    "    all_list_grid2 = pd.DataFrame(all_list_grid2, columns=columns)\n",
    "    all_list_grid2.to_csv(\"...\\\\P_LUD_PC_r\" + str(s) + \".csv\", index=False)\n",
    "    #all_list_grid2.to_csv(\"...\\\\P_LUD_PC_n\" + str(s) + \".csv\", index=False) #When replaced with 'lat + s', output as 'n'\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f64028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "\n",
    "# Environmental factors list\n",
    "column_list =[\"fg_\",\"hu_\",\"rr_\",\"qq_\",\"tg_\",\"total_evaporation_sum_\",\"volumetric_soil_water_layer_1_\",'soil_temperature_level_1_']\n",
    "\n",
    "# Geographic Zoning\n",
    "for s in range(1,5): # Move 1째 each time \n",
    "    mean_r=[]  \n",
    "    for lon in range(-15,35,5):\n",
    "        for lat in range(30,75,5):\n",
    "            # Select the phenology data of the corresponding area\n",
    "            filtered_df = df.query(f'{lon+s} <= Longitude <= {lon+5+s} and {lat} <= Latitude <= {lat+5}').reset_index() \n",
    "            #filtered_df = df.query(f'{lon} <= Longitude <= {lon+5} and {lat+s} <= Latitude <= {lat+5+s}').reset_index() #Replace 's' with 'lat' after iteration ends and run again\n",
    "            # Count the number of records (>200) of phenological patterns in the geographical grid\n",
    "            number_of_categories = (filtered_df.groupby('species_cluster')\n",
    "                                    .size()\n",
    "                                    .reset_index(name='count')\n",
    "                                    .query('count > 200')\n",
    "                                    .shape[0])\n",
    "            # If the required number of phenological patterns >= 10\n",
    "            if number_of_categories>=10:\n",
    "                #Iterate over each phenological pattern\n",
    "                all_list =[]\n",
    "                species_list = filtered_df['species_cluster'].unique()\n",
    "                scaler = StandardScaler()\n",
    "                for sp in species_list:\n",
    "                    MIC_list=[] \n",
    "                    filtered_df1=filtered_df[filtered_df[\"species_cluster\"]==sp].copy()\n",
    "                    if(filtered_df1['year'].nunique()>=10): #Filter phenological patterns with records >= 10 years\n",
    "                        for factor in column_list:  \n",
    "                            MIC = calculate_most_influential_column(factor, filtered_df1)\n",
    "                            MIC_list.append(MIC)#Record the optimal pre-season period for each environmental factor \n",
    "                        filtered_df1 =  filtered_df1.dropna(subset=MIC_list)\n",
    "                        for fc in MIC_list:\n",
    "                            # Remove annual trends\n",
    "                            y_residual = remove_annual_trends(filtered_df1,fc)\n",
    "                            # Standardise residuals\n",
    "                            filtered_df1[fc] = scaler.fit_transform(y_residual.values.reshape(-1, 1)) \n",
    "                    \n",
    "                        # Ridge Regression\n",
    "                        X = filtered_df1[MIC_list]\n",
    "                        y = filtered_df1['LUD'] #LUD/IOD\n",
    "                        ridge_model = Ridge(alpha=1.0)\n",
    "                        ridge_model.fit(X, y)\n",
    "                        # Records the results\n",
    "                        pc_list = []\n",
    "                        coefficients = ridge_model.coef_[0:len(MIC_list)]\n",
    "                        for coeff in coefficients:\n",
    "                            pc_list.append([coeff])\n",
    "                        all_list.append([sp,filtered_df1[\"order\"].unique()[0],pc_list])\n",
    "                \n",
    "                # Organise data\n",
    "                df_all = pd.DataFrame(all_list, columns=['sp','order','params']) \n",
    "                for i, fa in enumerate(column_list):\n",
    "                    df_all[fa + \"r\"] = df_all['params'].apply(lambda x: x[i][0])\n",
    "\n",
    "                # Calculate the average Ridge Regression correlation coefficients for a single geographical grid\n",
    "                columns_to_average = [fa + \"r\" for fa in column_list]\n",
    "                mean_r.append([lon + 2.5, lat + 2.5 + s, df_all[columns_to_average].mean().tolist()])\n",
    "    \n",
    "    #Organise and output data\n",
    "    all_list_grid = [item[:2] + item[2] for item in mean_r]  \n",
    "    columns = [\"lon\",\"lat\",\"fg_r\",\"hu_r\",\"rr_r\",\"qq_r\",\"tg_r\",\"total_evaporation_sum_r\",\"volumetric_soil_water_layer_1_r\",'soil_temperature_level_1_r']\n",
    "    all_list_grid = pd.DataFrame(all_list_grid, columns=columns)\n",
    "    all_list_grid.to_csv(\"...\\\\LUD_RC_n\" + str(s) + \".csv\", index=False) #When replaced with 'lat + s', output as 'n'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f12cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "\n",
    "# Environmental factors list\n",
    "column_list =[\"fg_\",\"hu_\",\"rr_\",\"qq_\",\"tg_\",\"total_evaporation_sum_\",\"volumetric_soil_water_layer_1_\",'soil_temperature_level_1_']\n",
    "\n",
    "# Geographic Zoning\n",
    "for s in range(1,5): # Move 1째 each time \n",
    "    mean_r=[]  \n",
    "    for lon in range(-15,35,5):\n",
    "        for lat in range(30,75,5):\n",
    "            # Select the phenology data of the corresponding area\n",
    "            filtered_df = df.query(f'{lon+s} <= Longitude <= {lon+5+s} and {lat} <= Latitude <= {lat+5}').reset_index() \n",
    "            #filtered_df = df.query(f'{lon} <= Longitude <= {lon+5} and {lat+s} <= Latitude <= {lat+5+s}').reset_index() #Replace 's' with 'lat' after iteration ends and run again\n",
    "            # Count the number of records (>200) of phenological patterns in the geographical grid\n",
    "            number_of_categories = (filtered_df.groupby('species_cluster')\n",
    "                                    .size()\n",
    "                                    .reset_index(name='count')\n",
    "                                    .query('count > 200')\n",
    "                                    .shape[0])\n",
    "            # If the required number of phenological patterns >= 10\n",
    "            if number_of_categories>=10:\n",
    "                #Iterate over each phenological pattern\n",
    "                all_list =[]\n",
    "                species_list = filtered_df['species_cluster'].unique()\n",
    "                scaler = StandardScaler()\n",
    "                for sp in species_list:\n",
    "                    MIC_list=[] \n",
    "                    filtered_df1=filtered_df[filtered_df[\"species_cluster\"]==sp].copy()\n",
    "                    if(filtered_df1['year'].nunique()>=10): #Filter phenological patterns with records >= 10 years\n",
    "                        for factor in column_list:  \n",
    "                            MIC = calculate_most_influential_column(factor, filtered_df1)\n",
    "                            MIC_list.append(MIC)#Record the optimal pre-season period for each environmental factor \n",
    "                        filtered_df1 =  filtered_df1.dropna(subset=MIC_list)\n",
    "                        for fc in MIC_list:\n",
    "                            # Remove annual trends\n",
    "                            y_residual = remove_annual_trends(filtered_df1,fc)\n",
    "                            # Standardise residuals\n",
    "                            filtered_df1[fc] = scaler.fit_transform(y_residual.values.reshape(-1, 1)) \n",
    "                        \n",
    "                        # Random forest\n",
    "                        pc_list =[]\n",
    "                        X = filtered_df1[MIC_list]\n",
    "                        y = filtered_df1['LUD'] #LUD/IOD\n",
    "                        rf_model = RandomForestRegressor(n_estimators=100)\n",
    "                        rf_model.fit(X, y)\n",
    "                        feature_importances = rf_model.feature_importances_\n",
    "                        importances = feature_importances[:len(MIC_list)]\n",
    "                        pc_list = [[importance] for importance in importances]\n",
    "                        all_list.append([sp, filtered_df1[\"order\"].unique()[0], pc_list])\n",
    "                \n",
    "                # Organise data\n",
    "                df_all = pd.DataFrame(all_list, columns=['sp','order','params']) \n",
    "                for i, fa in enumerate(column_list):\n",
    "                    df_all[fa + \"r\"] = df_all['params'].apply(lambda x: x[i][0])\n",
    "                # Calculate the average random forest feature importance for a single geographical grid\n",
    "                columns_to_average = [fa + \"r\" for fa in column_list]\n",
    "                mean_r.append([lon + 2.5 + s, lat + 2.5, df_all[columns_to_average].mean().tolist()])\n",
    "    \n",
    "    #Organise and output data\n",
    "    all_list_grid = [item[:2] + item[2] for item in mean_r]  \n",
    "    columns = [\"lon\",\"lat\",\"fg_r\",\"hu_r\",\"rr_r\",\"qq_r\",\"tg_r\",\"total_evaporation_sum_r\",\"volumetric_soil_water_layer_1_r\",'soil_temperature_level_1_r']\n",
    "    all_list_grid = pd.DataFrame(all_list_grid, columns=columns)\n",
    "    all_list_grid.to_csv(\"...\\\\LUD_RF_r\" + str(s) + \".csv\", index=False) #When replaced with 'lat + s', output as 'n'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
