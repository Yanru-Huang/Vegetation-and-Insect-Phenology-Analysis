{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc15256",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Insect multi-phenology patterns classification###\n",
    "\n",
    "#The code is written to categorise insect occurrence records based on their degree of voltinism.Insects with varying voltinism exhibit different phenological patterns \n",
    "# - either unimodal (single peak of activity) or multimodal (multiple peaks). \n",
    "#These patterns are critical for understanding insect population dynamics and community interactions.\n",
    "\n",
    "#Step:\n",
    "#(1) Create the probability density curve for each species occurrence records.\n",
    "#(2) Smooth the probability density curve using Gaussian and S-G filter.\n",
    "#(3) Determine whether the phenology is characterised by multiple peaks (using the find_peaks() function from scipy package).\n",
    "#(4) If it has the multi-peak phenology, K-means clustering is performed.The Silhouette Scores is used to find the optimal cluster number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09623c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load package\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import seaborn as sns\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca1e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch obtain data from .CSV files in a folder\n",
    "def list_csv_files(root_dir, csv_list):\n",
    "    # Use glob to find all .csv files in the directory and its subdirectories\n",
    "    csv_files = glob.glob(os.path.join(root_dir, '**/*.csv'), recursive=True)\n",
    "    csv_list.extend(csv_files)\n",
    "\n",
    "# Initialise an empty list\n",
    "list_csv = []\n",
    "\n",
    "# store result in list_csv\n",
    "list_csv_files(r\"...:\\Insects_occurrence_file_path\", list_csv) #Modified to the path where phenology data are stored "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba1a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise an empty DataFrame to store the result\n",
    "df_mode = pd.DataFrame(columns=[\"Species\", \"Gaussian\", \"S_G\", \"K_means\",\"keep_mode\"])\n",
    "\n",
    "# Loop through the list of CSV files\n",
    "for i in list_csv:\n",
    "    #Get species occurrence records\n",
    "    fpath, fname = os.path.split(i)\n",
    "    Species = fname.split(\".\")[0]\n",
    "    df_ins = pd.read_csv(i, sep=',')\n",
    "    \n",
    "    # create the probability density curve \n",
    "    # Group by every 14 days and count\n",
    "    total_count = len(df_ins)\n",
    "    df_ins['IOD_grouped'] = (df_ins['IOD'] - 1) // 14* 14 + 1  # Compute starting day of each week\n",
    "    grouped_counts = df_ins.groupby('IOD_grouped')['IOD'].agg('count').reset_index()\n",
    "    grouped_counts.columns = ['IOD_grouped', 'Count']\n",
    "\n",
    "    # Calculate the relative frequency for 14-day groups\n",
    "    grouped_counts['relative_frequency'] = grouped_counts['Count'] / total_count\n",
    "\n",
    "    # Smooth the probability density curve using Gaussian filter\n",
    "    sigma = 0.5\n",
    "    grouped_counts['Occurrences_Smoothed'] = gaussian_filter1d(grouped_counts['relative_frequency'], sigma=sigma)\n",
    "    peaks, _ = find_peaks(grouped_counts['Occurrences_Smoothed'], prominence=0.005,height=0.02)\n",
    "    Gaussian = len(peaks)\n",
    "\n",
    "    # Smooth the probability density curve using S-G filter\n",
    "    window_length = 5\n",
    "    polyorder = 3\n",
    "    grouped_counts['Occurrences_Smoothed_SG'] = savgol_filter(grouped_counts['relative_frequency'], window_length, polyorder)\n",
    "    peaks, _ = find_peaks(grouped_counts['Occurrences_Smoothed_SG'], prominence=0.005,height=0.02)\n",
    "    S_G = len(peaks)\n",
    "        \n",
    "    #Adjusted single/multiple phenological peak results for comparison with literature records\n",
    "    def adjust_pheno_pattern(species_list, current_S_G, current_Gaussian):\n",
    "        # Mapping species to their modes\n",
    "        mode_mapping = {\n",
    "            1:[\"Anthonomus pedicularius\",\"Batophila rubi\",\"Brachyderes incanus\",\"Cassida viridis\",\"Ceratapion onopordi\",\"Ceutorhynchus contractus\",\"Ceutorhynchus obstrictus\",\"Chaetocnema concinna\",\"Chrysolina fastuosa\",\"Chrysolina hyperici\",\"Crepidodera aurata\",\"Crepidodera aurea\",\"Crepidodera fulvicornis\",\"Dryocoetes autographus\",\"Exapion ulicis\",\"Exomias araneiformis\",\"Galerucella tenella\",\"Hylastes cunicularius\",\"Hylesinus varius\",\"Hypera nigrirostris\",\"Hypera postica\",\"Ischnopterapion virens\",\"Mecinus pyraster\",\"Otiorhynchus sulcatus\",\"Oulema melanopus\",\"Paracorymbia maculicornis\",\"Perapion marchicum\",\"Phaenops cyanea\",\"Phyllotreta vittula\",\"Psylliodes chrysocephalus\",\"Psylliodes napi\",\"Pyrrhalta viburni\",\"Sitona hispidulus\",\"Sitona sulcifrons\",\"Tachyerges salicis\",\"Trichius fasciatus\",\"Andrena fuscipes\",\"Anthidium punctatum\",\"Athalia circularis\",\"Bombus magnus\",\"Bombus monticola\",\"Bombus muscorum\",\"Bombus ruderarius\",\"Bombus sporadicus\",\"Bombus sylvarum\",\"Chelostoma campanularum\",\"Hylaeus communis\",\"Lasioglossum fratellum\",\"Lasioglossum fulvicorne\",\"Lasioglossum punctatissimum\",\"Lasioglossum sexstrigatum\",\"Lasioglossum zonulum\",\"Megachile ligniseca\",\"Osmia bicolor\",\"Panurgus banksianus\",\"Xylocopa violacea\",\"Abraxas sylvata\",\"Acrocercops brongniardella\",\"Aethes smeathmanniana\",\"Agonopterix propinquella\",\"Anania verbascalis\",\"Ancylis laetana\",\"Callimorpha dominula\",\"Dicallomera fascelina\",\"Eriogaster lanestris\",\"Euproctis chrysorrhoea\",\"Gonepteryx cleopatra\",\"Hadena albimacula\",\"Heliothis viriplaca\",\"Herminia tarsicrinalis\",\"Hipparchia statilinus\",\"Lasiocampa trifolii\",\"Lithophane lamda\",\"Lycia zonaria\",\"Phigalia pilosaria\",\"Pyla fusca\",\"Pyronia cecilia\",\"Saturnia pavonia\",\"Spiris striata\",\"Thaumetopoea processionea\"],\n",
    "            2:[\"Harpalus tardus\",\"Leptinotarsa decemlineata\",\"Plagiodera versicolora\",\"Lygus rugulipennis\", \"Bombus lapidarius\",\"Bombus lucorum\",\"Bombus pascuorum\",\"Bombus pratorum\",\"Halictus tumulorum\",\"Lasioglossum leucopus\",\"Lasioglossum morio\", \"Acleris comariana\",\"Acronicta auricoma\",\"Acronicta rumicis\",\"Actinotia polyodon\",\"Ancylis badiana\",\"Apatura ilia\",\"Asthena albulata\",\"Cameraria ohridella\",\"Clepsis spectrana\",\"Coenonympha pamphilus\",\"Colostygia pectinataria\",\"Craniophora ligustri\",\"Cyclophora linearia\",\"Diachrysia chrysitis\",\"Diacrisia sannio\",\"Drepana curvatula\",\"Dysstroma truncata\",\"Earias clorana\",\"Epirrhoe galiata\",\"Epirrhoe rivata\",\"Epirrhoe tristata\",\"Erynnis tages\",\"Eucarta virgo\",\"Eupithecia assimilata\",\"Evergestis forficalis\",\"Falcaria lacertinaria\",\"Furcula bifida\",\"Hadena bicruris\",\"Hylaea fasciaria\",\"Hypena proboscidalis\",\"Hypena rostralis\",\"Lampropteryx otregiata\",\"Lathronympha strigana\",\"Leptidea juvernica\",\"Lycaena dispar\",\"Lygephila viciae\",\"Lyonetia clerkella\",\"Lythria cruentaria\",\"Macaria alternata\",\"Melitaea phoebe\",\"Minoa murinata\",\"Mythimna pallens\",\"Notodonta dromedarius\",\"Ochropleura plecta\",\"Opisthograptis luteolata\",\"Orthonama vittata\",\"Pseudeustrotia candidula\",\"Pterostoma palpina\",\"Pyrausta purpuralis\",\"Rivula sericealis\",\"Scoliopteryx libatrix\",\"Scopula immorata\",\"Sideridis rivularis\",\"Thera obeliscata\"],\n",
    "            3:[\"Boloria dia\", \"Gymnoscelis rufifasciata\"]  \n",
    "        }\n",
    "        for mode, species in mode_mapping.items():\n",
    "            if any(sp in species for sp in species_list):\n",
    "                return mode, mode  \n",
    "        return current_S_G, current_Gaussian \n",
    "    \n",
    "    species_list = df_ins[\"species\"].unique()\n",
    "    S_G, Gaussian = adjust_pheno_pattern(species_list, S_G, Gaussian)\n",
    "\n",
    "    # If it is the single-peak phenology, K-means clustering is not performed.\n",
    "    if(S_G ==1 and Gaussian ==1):\n",
    "        K_means =\"U\"\n",
    "        keep_mode = 1\n",
    "        df=df_ins #Avoid changing the original data\n",
    "        #Remove outliers\n",
    "        mean=df[\"IOD\"].mean()\n",
    "        std = df[\"IOD\"].std()\n",
    "        filtered_df = df[abs(df['IOD'] - mean) < 3 * std] \n",
    "        #record result\n",
    "        df_mode =pd.concat([df_mode, pd.DataFrame({\"Species\": [Species], 'Gaussian': [Gaussian],\"S_G\":[S_G],\"K_means\":[K_means],\"keep_mode\":[keep_mode]})], axis=0)\n",
    "        filtered_df['Cluster']=1\n",
    "        filtered_df= filtered_df.drop(columns=['IOD_grouped'])\n",
    "        #output single-peak phenology occurrence record\n",
    "        filtered_df.to_csv(\"...\\\\your_path\\\\\"+Species+\"_S.csv\",encoding='utf-8',index=False) #Modified to the path where you want to store the result \n",
    "    \n",
    "    # If it is the multi-peak phenology, K-means clustering is performed.\n",
    "    else:\n",
    "        df= df_ins #Avoid changing the original data\n",
    "\n",
    "        #Standardise the data using StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        df_scaled = scaler.fit_transform(df[['IOD']])\n",
    "\n",
    "        #Apply K-Means and use the Silhouette Scores to find the optimal number of clusters\n",
    "        silhouette_scores = []\n",
    "        max_clusters = max(S_G,Gaussian)\n",
    "        range_clusters = range(2, max_clusters + 1)\n",
    "\n",
    "        for k in range_clusters:\n",
    "            kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10,random_state=42)\n",
    "            kmeans.fit(df_scaled)\n",
    "            silhouette_scores.append(silhouette_score(df_scaled, kmeans.labels_))\n",
    "\n",
    "        #Find the number of clusters that maximizes the silhouette_scores\n",
    "        optimal_index = np.argmax(silhouette_scores)\n",
    "        optimal_clusters = range_clusters[optimal_index]\n",
    "\n",
    "        K_means = optimal_clusters\n",
    "        \n",
    "        # Apply K-Means with the chosen number of clusters and add the cluster labels to the DataFrame\n",
    "        kmeans = KMeans(n_clusters=optimal_clusters, init='k-means++', random_state=42)\n",
    "        kmeans.fit(df_scaled)\n",
    "        df['Cluster'] = kmeans.labels_\n",
    "        \n",
    "        # Calculate the error (distance) between each data point and its corresponding cluster center\n",
    "        errors = np.linalg.norm(df_scaled - kmeans.cluster_centers_[kmeans.labels_], axis=1)\n",
    "        \n",
    "        # Calculate the mean and standard deviation of the errors\n",
    "        mean_error = np.mean(errors)\n",
    "        std_error = np.std(errors)\n",
    "        # Identify and exclude outliers \n",
    "        outlier_threshold = mean_error + 3 * std_error\n",
    "        outliers = np.where(errors > outlier_threshold)\n",
    "        df = df.drop(outliers[0])\n",
    "        \n",
    "        # Count the size of each cluster group\n",
    "        counts = df.groupby('Cluster').size()\n",
    "        # Delete cluster group with size less than 400\n",
    "        clusters_to_remove = counts[counts < 400].index.tolist()\n",
    "        keep_mode = K_means - len(clusters_to_remove)\n",
    "        df = df[~df['Cluster'].isin(clusters_to_remove)]\n",
    "        \n",
    "        #record multi-peak phenology result\n",
    "        df_mode =pd.concat([df_mode, pd.DataFrame({\"Species\": [Species], 'Gaussian': [Gaussian],\"S_G\":[S_G],\"K_means\":[K_means],\"keep_mode\":[keep_mode] })], axis=0)\n",
    "      \n",
    "        #Sort by generation\n",
    "        average_by_category = df.groupby('Cluster')['IOD'].mean().reset_index(name='C')\n",
    "        sorted_average_by_category = average_by_category.sort_values(by='C', ascending=True).reset_index(drop = True)\n",
    "\n",
    "        for j in range(0,len(sorted_average_by_category[\"Cluster\"])):\n",
    "            df_mode_data = (df[df[\"Cluster\"]==sorted_average_by_category[\"Cluster\"][j]]).reset_index(drop = True)\n",
    "            df_mode_data['Cluster']=j+1\n",
    "            df_mode_data=df_mode_data.drop(columns=['IOD_grouped'])\n",
    "            df_mode_data.to_csv(\"...\\\\your_path\\\\\"+Species+\"_\"+str(j+1)+\".csv\",encoding='utf-8',index=False) #Modified to the path where you want to store the result     \n",
    "        \n",
    "# Save df_mode to a CSV file if needed\n",
    "df_mode.to_csv(r'...\\\\your_path\\\\Mode_result.csv', index=False) #Modified to the path where you want to store the result     \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
